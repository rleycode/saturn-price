#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–π Saturn
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

def get_sample_category_urls():
    """–ü–æ–ª—É—á–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ sitemap"""
    
    sitemap_urls = [
        "https://nnv.saturn.net/sitemap.xml",
        "https://nnv.saturn.net/sitemaps/nnv.sitemap.xml"
    ]
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    all_product_urls = set()
    
    for sitemap_url in sitemap_urls:
        try:
            response = session.get(sitemap_url, timeout=15)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            
            urls = []
            for url_elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
                loc = url_elem.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                if loc is not None:
                    urls.append(loc.text)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º URL —Ç–æ–≤–∞—Ä–æ–≤
            product_urls = []
            for url in urls:
                if '/catalog/' in url:
                    catalog_part = url.split('/catalog/', 1)[1]
                    slash_count = catalog_part.count('/')
                    
                    if slash_count >= 2:
                        product_urls.append(url)
            
            all_product_urls.update(product_urls)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sitemap {sitemap_url}: {e}")
    
    return list(all_product_urls)[:5]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

def analyze_category_structure(urls):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    for i, url in enumerate(urls, 1):
        print(f"\n{'='*60}")
        print(f"üîç –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ò {i}: {url}")
        print(f"{'='*60}")
        
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.find('title')
            if title:
                print(f"üìÑ Title: {title.get_text(strip=True)}")
            
            # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Ç–æ–≤–∞—Ä–æ–≤
            selectors_to_check = [
                'div.h_s_list_categor_item_wrap',
                'div.catalog-item',
                'div.product-item',
                'div.item',
                '.product',
                '.goods-item',
                '[data-product]'
            ]
            
            print(f"\nüîç –ü–û–ò–°–ö –ö–û–ù–¢–ï–ô–ù–ï–†–û–í –¢–û–í–ê–†–û–í:")
            
            for selector in selectors_to_check:
                containers = soup.select(selector)
                print(f"   {selector}: {len(containers)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                if containers and len(containers) <= 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–µ–º–Ω–æ–≥–æ
                    for j, container in enumerate(containers[:3], 1):
                        print(f"      –≠–ª–µ–º–µ–Ω—Ç {j}: {container.name} class='{container.get('class', [])}'")
                        text_preview = container.get_text(strip=True)[:100]
                        print(f"      –¢–µ–∫—Å—Ç: {text_preview}...")
            
            # –ò—â–µ–º –∞—Ä—Ç–∏–∫—É–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()
            import re
            articles = re.findall(r'—Ç–æ–≤-(\d+)', page_text, re.IGNORECASE)
            print(f"\nüè∑Ô∏è  –ê–†–¢–ò–ö–£–õ–´ –í –¢–ï–ö–°–¢–ï: {len(set(articles))} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
            if articles:
                unique_articles = list(set(articles))[:10]
                print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(unique_articles)}")
            
            # –ò—â–µ–º —Ü–µ–Ω—ã
            price_matches = re.findall(r'(\d+[,.]?\d*)\s*‚ÇΩ', page_text)
            print(f"\nüí∞ –¶–ï–ù–´ –í –¢–ï–ö–°–¢–ï: {len(price_matches)} –Ω–∞–π–¥–µ–Ω–æ")
            if price_matches:
                unique_prices = list(set(price_matches))[:10]
                print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(unique_prices)}‚ÇΩ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ div —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏
            all_divs = soup.find_all('div', class_=True)
            class_counts = {}
            for div in all_divs:
                classes = div.get('class', [])
                for cls in classes:
                    if 'item' in cls.lower() or 'product' in cls.lower() or 'goods' in cls.lower():
                        class_counts[cls] = class_counts.get(cls, 0) + 1
            
            if class_counts:
                print(f"\nüì¶ –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ï –ö–õ–ê–°–°–´ –¢–û–í–ê–†–û–í:")
                for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                    print(f"   .{cls}: {count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            filename = f"debug_category_{i}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"\nüíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {url}: {e}")

def main():
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶ –ö–ê–¢–ï–ì–û–†–ò–ô")
    print("=" * 50)
    
    # –ü–æ–ª—É—á–∞–µ–º URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_urls = get_sample_category_urls()
    
    if not category_urls:
        print("‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ sitemap")
        return 1
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(category_urls)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    analyze_category_structure(category_urls)
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())
