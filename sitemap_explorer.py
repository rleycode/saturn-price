#!/usr/bin/env python3
"""
–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ sitemap –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–æ–≤–∞—Ä–∞–º Saturn
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse
import re
import logging
from typing import List, Set

class SaturnSitemapExplorer:
    
    def __init__(self):
        self.base_url = "https://msk.saturn.net"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.logger = logging.getLogger(__name__)
        
    def check_robots_txt(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç robots.txt –¥–ª—è –ø–æ–∏—Å–∫–∞ sitemap"""
        
        try:
            robots_url = f"{self.base_url}/robots.txt"
            self.logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º robots.txt: {robots_url}")
            
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                content = response.text
                print("üìÑ robots.txt –Ω–∞–π–¥–µ–Ω:")
                print(content[:500] + "..." if len(content) > 500 else content)
                
                # –ò—â–µ–º sitemap
                sitemap_urls = re.findall(r'Sitemap:\s*(.+)', content, re.IGNORECASE)
                if sitemap_urls:
                    print(f"\nüó∫Ô∏è –ù–∞–π–¥–µ–Ω—ã sitemap URLs:")
                    for url in sitemap_urls:
                        print(f"  - {url.strip()}")
                    return [url.strip() for url in sitemap_urls]
                else:
                    print("‚ùå Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ robots.txt")
            else:
                print(f"‚ùå robots.txt –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ robots.txt: {e}")
        
        return []
    
    def check_common_sitemap_urls(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ URL –¥–ª—è sitemap"""
        
        common_urls = [
            f"{self.base_url}/sitemap.xml",
            f"{self.base_url}/sitemap_index.xml",
            f"{self.base_url}/sitemaps/sitemap.xml",
            f"{self.base_url}/sitemap/sitemap.xml"
        ]
        
        found_sitemaps = []
        
        for url in common_urls:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω sitemap: {url}")
                    found_sitemaps.append(url)
                else:
                    print(f"‚ùå {url}: {response.status_code}")
            except Exception as e:
                print(f"‚ùå {url}: {e}")
        
        return found_sitemaps
    
    def parse_sitemap(self, sitemap_url: str) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç sitemap –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ç–æ–≤–∞—Ä–æ–≤"""
        
        try:
            self.logger.info(f"–ü–∞—Ä—Å–∏–º sitemap: {sitemap_url}")
            
            response = self.session.get(sitemap_url, timeout=15)
            response.raise_for_status()
            
            # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ XML
            try:
                root = ET.fromstring(response.content)
                
                # –ò—â–µ–º URL –≤ sitemap
                urls = []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º sitemap index
                for sitemap in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap'):
                    loc = sitemap.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                    if loc is not None:
                        urls.append(loc.text)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—ã—á–Ω—ã–π sitemap
                for url in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
                    loc = url.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                    if loc is not None:
                        urls.append(loc.text)
                
                print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –≤ sitemap")
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º URL —Ç–æ–≤–∞—Ä–æ–≤
                product_urls = []
                for url in urls:
                    if '/catalog/' in url and url.count('/') > 4:  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —Ç–æ–≤–∞—Ä—ã –∏–º–µ—é—Ç –≥–ª—É–±–æ–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                        product_urls.append(url)
                
                print(f"üõçÔ∏è –ò–∑ –Ω–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(product_urls)}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
                for i, url in enumerate(product_urls[:5]):
                    print(f"  {i+1}. {url}")
                
                return product_urls
                
            except ET.ParseError:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ XML")
                return []
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sitemap: {e}")
            return []
    
    def find_ajax_endpoints(self):
        """–ò—â–µ—Ç AJAX endpoints –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤"""
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞
            catalog_url = f"{self.base_url}/catalog/"
            response = self.session.get(catalog_url, timeout=15)
            response.raise_for_status()
            
            content = response.text
            
            # –ò—â–µ–º AJAX URL –≤ JavaScript
            ajax_patterns = [
                r'ajax["\']?\s*:\s*["\']([^"\']+)["\']',
                r'url["\']?\s*:\s*["\']([^"\']*ajax[^"\']*)["\']',
                r'["\']([^"\']*\/ajax\/[^"\']*)["\']',
                r'["\']([^"\']*\.php\?[^"\']*)["\']'
            ]
            
            found_endpoints = set()
            
            for pattern in ajax_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match and ('catalog' in match.lower() or 'product' in match.lower() or 'ajax' in match.lower()):
                        full_url = urljoin(self.base_url, match)
                        found_endpoints.add(full_url)
            
            if found_endpoints:
                print(f"üîç –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ AJAX endpoints:")
                for endpoint in list(found_endpoints)[:10]:
                    print(f"  - {endpoint}")
            else:
                print("‚ùå AJAX endpoints –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            return list(found_endpoints)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ AJAX endpoints: {e}")
            return []
    
    def explore_category_structure(self):
        """–ò—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ —Å–∞–π—Ç–µ"""
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            main_url = f"{self.base_url}/"
            response = self.session.get(main_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_links = set()
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–µ–Ω—é –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            selectors = [
                'a[href*="/catalog/"]',
                '.menu a',
                '.navigation a',
                '.category a',
                'nav a'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and '/catalog/' in href:
                        full_url = urljoin(self.base_url, href)
                        text = link.get_text(strip=True)
                        if text and len(text) > 2:
                            category_links.add((full_url, text))
            
            print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(category_links)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
            for i, (url, text) in enumerate(list(category_links)[:10]):
                print(f"  {i+1}. {text} - {url}")
            
            return list(category_links)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return []

def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    explorer = SaturnSitemapExplorer()
    
    print("üîç –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–• –°–ü–û–°–û–ë–û–í –î–û–°–¢–£–ü–ê –ö –¢–û–í–ê–†–ê–ú SATURN")
    print("="*70)
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º robots.txt
    print("\n1Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê ROBOTS.TXT")
    print("-" * 30)
    sitemap_urls = explorer.check_robots_txt()
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ URL sitemap
    print("\n2Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê –°–¢–ê–ù–î–ê–†–¢–ù–´–• SITEMAP URL")
    print("-" * 40)
    common_sitemaps = explorer.check_common_sitemap_urls()
    
    all_sitemaps = sitemap_urls + common_sitemaps
    
    # 3. –ü–∞—Ä—Å–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ sitemap
    if all_sitemaps:
        print(f"\n3Ô∏è‚É£ –ü–ê–†–°–ò–ù–ì SITEMAP ({len(all_sitemaps)} –Ω–∞–π–¥–µ–Ω–æ)")
        print("-" * 30)
        
        all_product_urls = []
        for sitemap_url in all_sitemaps[:3]:  # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—ã–µ 3
            product_urls = explorer.parse_sitemap(sitemap_url)
            all_product_urls.extend(product_urls)
        
        if all_product_urls:
            print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ sitemap: {len(all_product_urls)}")
    
    # 4. –ò—â–µ–º AJAX endpoints
    print(f"\n4Ô∏è‚É£ –ü–û–ò–°–ö AJAX ENDPOINTS")
    print("-" * 25)
    ajax_endpoints = explorer.find_ajax_endpoints()
    
    # 5. –ò—Å—Å–ª–µ–¥—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    print(f"\n5Ô∏è‚É£ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô")
    print("-" * 25)
    categories = explorer.explore_category_structure()
    
    # –í—ã–≤–æ–¥—ã
    print(f"\nüí° –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("="*30)
    
    if all_sitemaps:
        print("‚úÖ –ù–∞–π–¥–µ–Ω—ã sitemap - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URL —Ç–æ–≤–∞—Ä–æ–≤")
    else:
        print("‚ùå Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    if ajax_endpoints:
        print("‚úÖ –ù–∞–π–¥–µ–Ω—ã AJAX endpoints - –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤")
    else:
        print("‚ùå AJAX endpoints –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    if categories:
        print("‚úÖ –ù–∞–π–¥–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ - –æ–±—Ö–æ–¥–∏—Ç–µ –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ç–¥–µ–ª—å–Ω–æ")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ catalog_crawler –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    else:
        print("‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    main()
