#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã sitemap_parser - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç URL –∏–∑ sitemap
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
from urllib.parse import urljoin

def get_sample_urls_from_sitemap():
    """–ü–æ–ª—É—á–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã URL –∏–∑ sitemap –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    sitemap_urls = [
        "https://nnv.saturn.net/sitemap.xml",
        "https://nnv.saturn.net/sitemaps/nnv.sitemap.xml"
    ]
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    all_product_urls = set()
    
    for sitemap_url in sitemap_urls:
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º sitemap: {sitemap_url}")
            
            response = session.get(sitemap_url, timeout=15)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML
            root = ET.fromstring(response.content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ URL
            urls = []
            for url_elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
                loc = url_elem.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                if loc is not None:
                    urls.append(loc.text)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º URL —Ç–æ–≤–∞—Ä–æ–≤
            product_urls = []
            for url in urls:
                if '/catalog/' in url:
                    catalog_part = url.split('/catalog/', 1)[1]
                    slash_count = catalog_part.count('/')
                    
                    if slash_count >= 2:
                        product_urls.append(url)
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(product_urls)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ {sitemap_url}")
            all_product_urls.update(product_urls)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sitemap {sitemap_url}: {e}")
    
    return list(all_product_urls)

def analyze_product_urls(urls, sample_size=10):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—Ä–∞–∑—Ü—ã URL —Ç–æ–≤–∞—Ä–æ–≤"""
    
    print(f"\nüîç –ê–ù–ê–õ–ò–ó {sample_size} –û–ë–†–ê–ó–¶–û–í URL:")
    print("=" * 50)
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    # –ë–µ—Ä–µ–º –æ–±—Ä–∞–∑—Ü—ã URL
    sample_urls = urls[:sample_size]
    
    for i, url in enumerate(sample_urls, 1):
        print(f"\n{i}. URL: {url}")
        
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title = soup.find('title')
            if title:
                print(f"   üìÑ Title: {title.get_text(strip=True)[:100]}")
            
            # –ò—â–µ–º –∞—Ä—Ç–∏–∫—É–ª —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            article_found = False
            
            # –°–ø–æ—Å–æ–± 1: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            article_selectors = [
                'span.article',
                '.product-article', 
                '[data-article]',
                'p.h_s_list_categor_item_articul'
            ]
            
            for selector in article_selectors:
                article_elem = soup.select_one(selector)
                if article_elem:
                    article_text = article_elem.get_text(strip=True)
                    if '—Ç–æ–≤-' in article_text:
                        print(f"   üè∑Ô∏è  –ê—Ä—Ç–∏–∫—É–ª ({selector}): {article_text}")
                        article_found = True
                        break
                    elif article_elem.get('data-article'):
                        article_text = article_elem.get('data-article')
                        if '—Ç–æ–≤-' in article_text:
                            print(f"   üè∑Ô∏è  –ê—Ä—Ç–∏–∫—É–ª (data-article): {article_text}")
                            article_found = True
                            break
            
            # –°–ø–æ—Å–æ–± 2: –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not article_found:
                page_text = soup.get_text()
                article_matches = re.findall(r'—Ç–æ–≤-(\d+)', page_text, re.IGNORECASE)
                if article_matches:
                    print(f"   üè∑Ô∏è  –ê—Ä—Ç–∏–∫—É–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ: {article_matches}")
                    article_found = True
            
            if not article_found:
                print(f"   ‚ùå –ê—Ä—Ç–∏–∫—É–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—É
            price_selectors = [
                '.price',
                '.product-price',
                '[data-price]',
                'span.shopping_cart_goods_list_item_sum_item'
            ]
            
            price_found = False
            for selector in price_selectors:
                price_elem = soup.select_one(selector)
                if price_elem:
                    if price_elem.get('data-price'):
                        print(f"   üí∞ –¶–µ–Ω–∞ ({selector}): {price_elem.get('data-price')}‚ÇΩ")
                        price_found = True
                        break
                    else:
                        price_text = price_elem.get_text(strip=True)
                        price_match = re.search(r'(\d+[,.]?\d*)', price_text)
                        if price_match:
                            print(f"   üí∞ –¶–µ–Ω–∞ ({selector}): {price_match.group(1)}‚ÇΩ")
                            price_found = True
                            break
            
            if not price_found:
                # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ç–µ–∫—Å—Ç–µ
                page_text = soup.get_text()
                price_match = re.search(r'(\d+[,.]?\d*)\s*‚ÇΩ', page_text)
                if price_match:
                    print(f"   üí∞ –¶–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {price_match.group(1)}‚ÇΩ")
                else:
                    print(f"   ‚ùå –¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç –∏–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            if response.history:
                print(f"   üîÑ –†–µ–¥–∏—Ä–µ–∫—Ç: {response.url}")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

def check_url_patterns(urls):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã URL"""
    
    print(f"\nüìã –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í URL:")
    print("=" * 30)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    patterns = {}
    
    for url in urls[:50]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 50
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –ø–æ—Å–ª–µ /catalog/
        if '/catalog/' in url:
            path = url.split('/catalog/', 1)[1]
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞)
            path_parts = path.split('/')
            if len(path_parts) >= 3:
                category_path = '/'.join(path_parts[:-1])
                if category_path not in patterns:
                    patterns[category_path] = []
                patterns[category_path].append(url)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(patterns)} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
    
    for i, (pattern, urls_in_pattern) in enumerate(list(patterns.items())[:10], 1):
        print(f"{i}. {pattern} ({len(urls_in_pattern)} —Ç–æ–≤–∞—Ä–æ–≤)")
        if len(urls_in_pattern) <= 3:
            for url in urls_in_pattern:
                print(f"   - {url}")

def main():
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê SITEMAP_PARSER")
    print("=" * 40)
    
    # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ sitemap
    product_urls = get_sample_urls_from_sitemap()
    
    if not product_urls:
        print("‚ùå –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ sitemap")
        return 1
    
    print(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(product_urls)}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã URL
    check_url_patterns(product_urls)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü
    analyze_product_urls(product_urls, sample_size=10)
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())
